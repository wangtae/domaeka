"""
LOA.i ì¹´ì¹´ì˜¤í†¡ ë´‡ ì‹œìŠ¤í…œì— ë¸”ë£¸ë²„ê·¸ í¬ë¡¤ëŸ¬ í†µí•© ì½”ë“œ (ë¶„í•  ë°œì†¡ ë²„ì „)
services/bloomberg_service.py íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì‚¬ìš©
"""

import aiohttp
import asyncio
import json
import os
import re
import random

from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List, Optional, Any

# ì½”ì–´ ëª¨ë“ˆ ì„í¬íŠ¸
from core.logger import logger
# from core.globals import room_to_writer  # ë ˆê±°ì‹œ ì½”ë“œ ì œê±°

# LLM ì„œë¹„ìŠ¤ ì„í¬íŠ¸
from services.llm_fallback_service import call_llm_with_fallback

from core import globals as g


class BloombergScraper:
    """
    ë¸”ë£¸ë²„ê·¸ ì½”ë¦¬ì•„ì˜ 'ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆ' ê¸°ì‚¬ë¥¼ ìŠ¤í¬ë©í•˜ê³ 
    ì¹´ì¹´ì˜¤í†¡ ë°©ì— ë¶„í• í•˜ì—¬ ë°œì†¡í•˜ëŠ” í´ë˜ìŠ¤
    """

    def __init__(self):
        self.blog_url = "https://www.bloomberg.co.kr/blog/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://www.bloomberg.co.kr/",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0"
        }
        self.timeout = aiohttp.ClientTimeout(total=15)  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.article_data = None  # íŒŒì‹±ëœ ê¸°ì‚¬ ë°ì´í„° ì €ì¥

    async def fetch_page(self, url: str) -> Optional[str]:
        """ì›¹í˜ì´ì§€ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ"""
        try:
            async with aiohttp.ClientSession(timeout=self.timeout) as session:
                async with session.get(url, headers=self.headers) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.error(f"Error fetching {url}: Status code {response.status}")
                        return None
        except Exception as e:
            logger.error(f"Error during HTTP request: {e}")
            return None

    async def get_latest_issue_url(self) -> Optional[str]:
        """ë¸”ë¡œê·¸ ë©”ì¸ í˜ì´ì§€ì—ì„œ 'ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆ'ë¡œ ì‹œì‘í•˜ëŠ” ê°€ì¥ ìµœì‹  ê¸°ì‚¬ URLì„ ì°¾ëŠ” ë©”ì„œë“œ"""
        html = await self.fetch_page(self.blog_url)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # ëª¨ë“  ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ìŒ
        article_links = soup.select(".media-body h3.h3-regular-8 a")

        # 'ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆ'ë¡œ ì‹œì‘í•˜ëŠ” ë§í¬ ì°¾ê¸°
        for link in article_links:
            title = link.text.strip()
            if title.startswith(("ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆ", "ì˜¤ëŠ˜ 5ê°€ì§€ ì´ìŠˆ")):
                article_url = link.get('href')
                if article_url:
                    logger.info(f"Found latest issue: {title} at {article_url}")
                    return article_url

        logger.warning("No 'Today's 5 Issues' article found")
        return None

    def _extract_article_date(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
        try:
            date_div = soup.select_one(".article__date .bbg-row--content div")
            if date_div:
                date_text = date_div.text.strip()
                logger.info(f"Extracted article date: {date_text}")
                return date_text
            return None
        except Exception as e:
            logger.error(f"Error extracting date: {e}")
            return None

    def _is_today_article(self, date_str: str) -> bool:
        """ê¸°ì‚¬ ë‚ ì§œê°€ ì˜¤ëŠ˜ì¸ì§€ í™•ì¸í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # ë‚ ì§œ í˜•ì‹: YYYY.MM.DD
            article_date = datetime.strptime(date_str, "%Y.%m.%d").date()
            today = datetime.now().date()

            # ì‹¤ì œ ìš´ì˜ ì‹œ ì‚¬ìš©í•  ì½”ë“œ
            return article_date == today
        except Exception as e:
            logger.error(f"Error parsing date: {e}")
            return False

    def _extract_article_sections(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ê¸°ì‚¬ì˜ ì œëª©, ìš”ì•½, ë³¸ë¬¸ ì„¹ì…˜ì„ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
        result = {
            "title": "",
            "summary": "",
            "issues": []
        }

        # 1. ì œëª© ì¶”ì¶œ
        title_elem = soup.select_one("h1.h1-regular-7")
        if title_elem:
            result["title"] = title_elem.text.strip()

        # 2. ìš”ì•½ ì¶”ì¶œ
        article_content = soup.select_one(".article__content")

        if article_content:
            # "ë‹¤ìŒì€ ì‹œì¥ ì°¸ê°€ìë“¤ì´ ê´€ì‹¬ì„ ê°€ì§ˆ ë§Œí•œ ì£¼ìš” ì´ìŠˆë“¤ì´ë‹¤." ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½ ì¶”ì¶œ
            paragraphs = article_content.select("p")

            summary_paragraphs = []
            reached_issues_marker = False

            for p in paragraphs:
                text = p.text.strip()
                if not text:
                    continue

                if "ë‹¤ìŒì€ ì‹œì¥ ì°¸ê°€ìë“¤ì´ ê´€ì‹¬ì„ ê°€ì§ˆ ë§Œí•œ ì£¼ìš” ì´ìŠˆë“¤ì´ë‹¤" in text:
                    reached_issues_marker = True
                    break

                if "(ë¸”ë£¸ë²„ê·¸)" in text or len(summary_paragraphs) < 2:
                    summary_paragraphs.append(text)

            result["summary"] = "\n\n".join(summary_paragraphs)

            # 3. h3 íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ìŠˆ ì¶”ì¶œ - ê°€ì¥ ì§ê´€ì ì´ê³  ì•ˆì •ì ì¸ ë°©ë²•
            h3_tags = article_content.select("h3")
            logger.info(f"Found {len(h3_tags)} h3 tags in the article")

            for h3 in h3_tags:
                # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ h3 íƒœê·¸ëŠ” ê±´ë„ˆë›°ê¸°
                if h3.find("img"):
                    continue

                title = h3.text.strip()

                # ì œëª©ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if not title or len(title) < 3:
                    continue

                content = ""
                next_elem = h3.next_sibling

                # ì´ h3ì™€ ë‹¤ìŒ h3 ì‚¬ì´ì˜ ëª¨ë“  p íƒœê·¸ ë‚´ìš© ìˆ˜ì§‘
                while next_elem:
                    if hasattr(next_elem, 'name'):
                        if next_elem.name == 'h3':  # ë‹¤ìŒ h3ë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨
                            break
                        elif next_elem.name == 'p':
                            p_text = next_elem.text.strip()
                            if p_text:
                                if content:
                                    content += "\n\n"
                                content += p_text
                    next_elem = next_elem.next_sibling

                # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                if content:
                    result["issues"].append({
                        "title": title,
                        "content": content
                    })
                    logger.info(f"Added issue with title: {title[:30]}...")

        # ë§Œì•½ h3 íƒœê·¸ë¡œ ì¶©ë¶„í•œ ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°ì—ë§Œ ëŒ€ì²´ ë°©ë²• ì‹œë„
        if len(result["issues"]) < 5:
            logger.warning(f"Only found {len(result['issues'])} issues with h3 tags, trying alternative methods")

            # í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ íŒ¨í„´ ì°¾ê¸°
            full_text = article_content.get_text(separator="\n", strip=True) if article_content else ""

            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ë½ íŒ¨í„´ ì°¾ê¸°
            issue_pattern = r'(\d+\.\s*[^.\n]{3,100})\s*\n+([^1-5].*?)(?=\n\d+\.\s*|$)'
            matches = re.findall(issue_pattern, full_text, re.DOTALL)

            # íŒ¨í„´ìœ¼ë¡œ ì´ìŠˆë¥¼ ì°¾ì•˜ë‹¤ë©´ ê¸°ì¡´ ì´ìŠˆë¥¼ ëŒ€ì²´
            if matches and len(matches) >= 3:
                result["issues"] = []  # ê¸°ì¡´ ì´ìŠˆ ë¦¬ì…‹
                for title, content in matches:
                    result["issues"].append({
                        "title": title.strip(),
                        "content": content.strip()
                    })

        # ì´ìŠˆ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ì—¬ê¸°ì„œëŠ” h3 íƒœê·¸ ìˆœì„œëŒ€ë¡œ)
        # ê°•ì œë¡œ 5ê°œ ì´ìŠˆ í•­ëª© ìƒì„± - ë¶€ì¡±í•œ ê²½ìš°ì—ë§Œ
        if len(result["issues"]) < 5:
            current_count = len(result["issues"])
            logger.warning(f"Could only extract {current_count} issues, adding placeholder issues")

            for i in range(current_count, 5):
                result["issues"].append({
                    "title": f"ì´ìŠˆ {i + 1}",
                    "content": "ë¸”ë£¸ë²„ê·¸ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì›ë¬¸ì„ ì°¸ì¡°í•˜ì„¸ìš”."
                })

        # ìµœëŒ€ 5ê°œ ì´ìŠˆë§Œ ì‚¬ìš©
        return {
            "title": result["title"],
            "summary": result["summary"],
            "issues": result["issues"][:5]
        }

    def _extract_structured_issues(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """êµ¬ì¡°ì ì¸ ë°©ë²•ìœ¼ë¡œ ì´ìŠˆë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ"""
        issues = []

        # ì£¼ìš” ì´ìŠˆ í—¤ë”ë“¤ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ íŠ¹ì • íŒ¨í„´ì˜ h3 íƒœê·¸)
        headers = []
        for h3 in soup.select(".wpb_wrapper h3"):
            text = h3.text.strip()
            if text and len(text.split("â€¦")) > 1:
                headers.append(h3)

        # ê° í—¤ë”ì— ëŒ€í•´ ë‚´ìš© ì¶”ì¶œ
        for i, header in enumerate(headers):
            title = header.text.strip()
            content = ""

            # ë‹¤ìŒ í—¤ë”ê¹Œì§€ì˜ ë‚´ìš© ì¶”ì¶œ
            current = header.next_sibling
            while current and (i == len(headers) - 1 or current != headers[i + 1]):
                if hasattr(current, 'name') and current.name == 'p':
                    content += current.text.strip() + "\n\n"
                current = current.next_sibling

            if title and content:
                issues.append({
                    "title": title,
                    "content": content.strip()
                })

        # ë°©ë²• 3: h3 íƒœê·¸ ë‚´ìš©ê³¼ ë‹¤ìŒ p íƒœê·¸ ê·¸ë£¹ ì°¾ê¸°
        if len(issues) < 5:
            all_h3 = soup.select(".wpb_wrapper h3")
            if len(all_h3) >= 5:  # ìµœì†Œ 5ê°œì˜ h3 íƒœê·¸ê°€ ìˆì–´ì•¼ í•¨
                issues = []
                for h3 in all_h3[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì‚¬ìš©
                    title = h3.text.strip()

                    # ì´ h3 ë‹¤ìŒì— ì˜¤ëŠ” ëª¨ë“  p íƒœê·¸ ì°¾ê¸°
                    content_paras = []
                    next_elem = h3.next_sibling
                    while next_elem:
                        if hasattr(next_elem, 'name'):
                            if next_elem.name == 'h3':  # ë‹¤ìŒ h3ë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨
                                break
                            elif next_elem.name == 'p':
                                content_paras.append(next_elem.text.strip())
                        next_elem = next_elem.next_sibling

                    # p íƒœê·¸ê°€ ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ p íƒœê·¸ë“¤ì„ ì°¾ê¸°
                    if not content_paras and h3.parent:
                        next_elem = h3.parent.next_sibling
                        while next_elem:
                            if hasattr(next_elem, 'name') and next_elem.name == 'p':
                                content_paras.append(next_elem.text.strip())
                                if len(content_paras) >= 2:  # ìµœëŒ€ 2ê°œì˜ ë‹¨ë½ë§Œ ê°€ì ¸ì˜´
                                    break
                            next_elem = next_elem.next_sibling

                    content = "\n\n".join(content_paras)
                    if title and content:
                        issues.append({
                            "title": title,
                            "content": content
                        })

        # ë°©ë²• 4: ë§ˆì§€ë§‰ ë°©ë²•ìœ¼ë¡œ ë³¸ë¬¸ì—ì„œ ì£¼ìš” h3 íƒœê·¸ì™€ ê·¸ í›„ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        if len(issues) < 5:
            all_content = soup.select_one(".article__content")
            if all_content:
                # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ
                all_text = all_content.get_text(separator="\n", strip=True)

                # ì •ê·œì‹ìœ¼ë¡œ íŒ¨í„´ ì°¾ê¸° (ì´ìŠˆ ì œëª©ê³¼ ë‚´ìš©)
                patterns = [
                    # íŒ¨í„´ 1: "ì œëª©â€¦" í˜•íƒœ
                    r'([^\n]+â€¦)\s*\n+([^\n].*?)(?=\n[^\n]+â€¦|\Z)',
                    # íŒ¨í„´ 2: h3 íƒœê·¸ ë‚´ìš© í˜•íƒœ
                    r'([^\n]{5,50})\s*\n+([^\n].*?)(?=\n[^\n]{5,50}\s*\n|\Z)'
                ]

                for pattern in patterns:
                    matches = re.findall(pattern, all_text, re.DOTALL)
                    if len(matches) >= 5:
                        issues = []
                        for title, content in matches[:5]:
                            issues.append({
                                "title": title.strip(),
                                "content": content.strip()
                            })
                        break

        return issues

    async def parse_article(self, article_url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë‚´ìš©ì„ íŒŒì‹±í•˜ëŠ” ë©”ì„œë“œ"""
        html = await self.fetch_page(article_url)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        # ê¸°ì‚¬ ë‚ ì§œ í™•ì¸
        date_str = self._extract_article_date(soup)
        if not date_str:
            logger.error("Could not extract article date")
            return None

        # ì˜¤ëŠ˜ ê¸°ì‚¬ì¸ì§€ í™•ì¸
        if not self._is_today_article(date_str):
            logger.info(f"Article date {date_str} is not today, skipping")
            return None

        # ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
        article_sections = self._extract_article_sections(soup)

        # ì¶©ë¶„í•œ ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê²½ê³ 
        if len(article_sections["issues"]) < 5:
            logger.warning(f"Could only extract {len(article_sections['issues'])} issues instead of 5")

            # ìˆ˜ë™ ì´ìŠˆ ìƒì„± (ë¶€ì¡±í•œ ë§Œí¼)
            for i in range(len(article_sections["issues"]), 5):
                article_sections["issues"].append({
                    "title": f"ì´ìŠˆ {i + 1}",
                    "content": "ì´ìŠˆ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                })

        # íŒŒì‹±ëœ ë°ì´í„° ì €ì¥
        self.article_data = {
            "date": date_str,
            "title": article_sections["title"],
            "url": article_url,
            "summary": article_sections["summary"],
            "issues": article_sections["issues"][:5]  # ìµœëŒ€ 5ê°œë§Œ ì‚¬ìš©
        }

        return self.article_data

    def format_for_kakao(self, article_data: Dict[str, Any]) -> List[str]:
        """
        ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° í¬ë§·íŒ…í•˜ì—¬ 7ê°œ ë©”ì‹œì§€ë¡œ ë¶„í• í•˜ëŠ” ë©”ì„œë“œ
        1. ì œëª© + ë§í¬
        2. ìš”ì•½
        3~7. ê° ì´ìŠˆë³„ ë©”ì‹œì§€
        """
        if not article_data:
            return ["ìµœì‹  ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]

        messages = []

        # 1. ì œëª© + ë§í¬ë§Œ í¬í•¨í•œ ì²« ë©”ì‹œì§€
        title_message = f"ğŸ“° {article_data['title']}\n\n"
        title_message += f"ğŸ”— ì›ë¬¸ ë³´ê¸°: {article_data['url']}"
        messages.append(title_message)

        # 2. ìš”ì•½ë§Œ í¬í•¨í•œ ë‘ ë²ˆì§¸ ë©”ì‹œì§€
        summary_message = f"{article_data['summary']}"
        messages.append(summary_message)

        # 3~7. ê° ì´ìŠˆë³„ ë©”ì‹œì§€
        for i, issue in enumerate(article_data['issues'], 1):
            if i > 5:  # ìµœëŒ€ 5ê°œ ì´ìŠˆ
                break

            issue_message = f"ğŸ“Œ {i}. {issue['title']}\n\n"
            issue_message += f"{issue['content']}"
            messages.append(issue_message)

        # 5ê°œ ì´ìŠˆê°€ ì•ˆ ë˜ëŠ” ê²½ìš°, ë¹ˆ ë©”ì‹œì§€ë¡œ ì±„ìš°ê¸°
        while len(messages) < 7:  # ì²« ë‘ ë©”ì‹œì§€ + 5ê°œ ì´ìŠˆ = 7ê°œ
            messages.append(f"âš ï¸ ì´ìŠˆ {len(messages) - 2}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        return messages

    async def create_article_summary(self) -> str:
        """
        íŒŒì‹±ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ í†µí•´ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ
        """
        try:
            if not self.article_data:
                logger.warning("[ARTICLE_SUMMARY] No article data available")
                return ""

            # LLMì— ì „ë‹¬í•  ì „ì²´ ê¸°ì‚¬ ë‚´ìš© êµ¬ì„±
            article_content = f"{self.article_data['title']}\n\n"
            article_content += f"{self.article_data['summary']}\n\n"

            for i, issue in enumerate(self.article_data['issues'], 1):
                article_content += f"ì´ìŠˆ {i}: {issue['title']}\n"
                article_content += f"{issue['content']}\n\n"

            # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒì€ ë¸”ë£¸ë²„ê·¸ ì½”ë¦¬ì•„ì˜ 'ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆ' ê¸°ì‚¬ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ìš”ì  5ê°œë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{article_content}

ìš”ì•½ì€ 5ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸ë¡œ ì‘ì„±í•˜ë˜, ê° í¬ì¸íŠ¸ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = {
                "bot_name": message.get("bot_name", ""),
                "channel_id": None,
                "user_hash": None,
                "sender": None
            }
            system_prompt = '''
            ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. {ìš”ì•½} ì´ë¶€ë¶„ì´ ìš”ì•½ ë‚´ìš©ì´ê³  ë‚˜ë¨¸ì§€ëŠ” í…œí”Œë¦¿ì…ë‹ˆë‹¤.
            
            ğŸ“‘ ê¸°ì‚¬ ìš”ì•½!
            ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ 5ê°€ì§€ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ë“œë¦´ê²Œìš”. ğŸ˜Š
            
            1. {ìš”ì•½1}
            
            2. {ìš”ì•½2}
            
            3. {ìš”ì•½3}
            
            4. {ìš”ì•½4}
            
            5. {ìš”ì•½5}
            
            ê¶ê¸ˆí•œ ì ì´ ë” ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”! ğŸŒŸ
            '''

            providers = [
                {
                    "name": "openai",
                    "timeout": 30,
                    "model": "gpt-4o",
                    "retry": 0,
                    "system_prompt": system_prompt
                },
                {
                    "name": "gemini",
                    "model": "gemini-1.5-pro",
                    "timeout": 30,
                    "retry": 0,
                    "system_prompt": system_prompt
                },
                {
                    "name": "grok",
                    "model": "grok-3-latest",
                    "timeout": 30,
                    "retry": 0,
                    "system_prompt": system_prompt
                }
            ]

            # LLM í˜¸ì¶œí•˜ì—¬ ìš”ì•½ ìƒì„±
            summary = await call_llm_with_fallback(context, prompt)

            if not summary or summary.startswith("[ERROR]"):
                logger.warning(f"[ARTICLE_SUMMARY] LLM failed to generate summary: {summary}")
                # ê¸°ë³¸ ìš”ì•½ ì œê³µ
                return "\n\nğŸ“‘ ê¸°ì‚¬ ìš”ì•½:\nâ€¢ ì˜¤ëŠ˜ì˜ ì£¼ìš” ì´ìŠˆëŠ” í†µí™” ì •ì±…, í™˜ìœ¨ ë³€ë™, í…ŒìŠ¬ë¼ ì£¼ê°€ í•˜ë½, ê·¸ë¦¬ê³  ê¸€ë¡œë²Œ ê¸ˆìœµì‹œì¥ ë³€í™”ì…ë‹ˆë‹¤."

            # ìš”ì•½ ê²°ê³¼ ì •ë¦¬
            return f"\n\nğŸ“‘ ê¸°ì‚¬ ìš”ì•½:\n\n{summary}"

        except Exception as e:
            logger.error(f"[ARTICLE_SUMMARY_ERROR] ê¸°ì‚¬ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ìš”ì•½ ì œê³µ
            return "\n\nğŸ“‘ ê¸°ì‚¬ ìš”ì•½:\nâ€¢ ì˜¤ëŠ˜ì˜ ì£¼ìš” ì´ìŠˆëŠ” í†µí™” ì •ì±…, í™˜ìœ¨ ë³€ë™, í…ŒìŠ¬ë¼ ì£¼ê°€ í•˜ë½, ê·¸ë¦¬ê³  ê¸€ë¡œë²Œ ê¸ˆìœµì‹œì¥ ë³€í™”ì…ë‹ˆë‹¤."

    async def get_latest_issues_split(self, include_summary=False) -> List[str]:
        """
        ìµœì‹  ì´ìŠˆë¥¼ ê°€ì ¸ì™€ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œ
        include_summary: ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ê¸°ì‚¬ ìš”ì•½ í¬í•¨ ì—¬ë¶€
        """
        article_url = await self.get_latest_issue_url()
        if not article_url:
            logger.error("ìµœì‹  ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # ì²« í˜ì´ì§€ì™€ ê¸°ì‚¬ í˜ì´ì§€ ì ‘ê·¼ ì‚¬ì´ì— ë”œë ˆì´ ì¶”ê°€
        await asyncio.sleep(random.uniform(3.7, 7.3))  # 2~4ì´ˆ ëœë¤ ë”œë ˆì´

        article_data = await self.parse_article(article_url)
        if not article_data:
            logger.error("ì˜¤ëŠ˜ì ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        messages = self.format_for_kakao(article_data)

        # ë§ˆë¬´ë¦¬ ë©”ì‹œì§€ ê¸°ë³¸ê°’
        closing_message = "ë¸”ë£¸ë²„ê·¸ ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆì…ë‹ˆë‹¤."

        # include_summaryê°€ Trueì¼ ê²½ìš° ìš”ì•½ ì¶”ê°€
        if include_summary:
            article_summary = await self.create_article_summary()
            if article_summary:
                closing_message += article_summary

        # ë§ˆë¬´ë¦¬ ë©”ì‹œì§€ ì¶”ê°€
        messages.append(closing_message)

        return messages


class BloombergService:
    """
    LOA.i ì¹´ì¹´ì˜¤í†¡ ë´‡ ì‹œìŠ¤í…œê³¼ ë¸”ë£¸ë²„ê·¸ ìŠ¤í¬ë˜í¼ë¥¼ ì—°ë™í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    """

    def __init__(self):
        self.scraper = BloombergScraper()
        self.schedule_path = g.JSON_CONFIG_FILES["schedule_rooms"]

    async def load_schedule_rooms(self) -> Dict[str, Dict[str, Dict]]:
        """ìŠ¤ì¼€ì¤„ ì„¤ì • íŒŒì¼ì—ì„œ ë°© ì •ë³´ë¥¼ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ"""
        try:
            if os.path.exists(self.schedule_path):
                with open(self.schedule_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.error(f"Schedule file not found: {self.schedule_path}")
                return {}
        except Exception as e:
            logger.error(f"Error loading schedule rooms: {e}")
            return {}

    def _get_rooms_for_bloomberg(self, schedule_data: Dict) -> List[Dict[str, str]]:
        """ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ë°œì†¡í•  ë°© ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ"""
        rooms = []
        try:
            for bot_name, bot_data in schedule_data.items():
                for channel_id, channel_data in bot_data.items():
                    # ìŠ¤ì¼€ì¤„ ë©”ì‹œì§€ì— "# ë¸”ë£¸ë²„ê·¸" ëª…ë ¹ì–´ê°€ ìˆëŠ” ë°© ì°¾ê¸°
                    if "schedules" in channel_data:
                        for schedule in channel_data["schedules"]:
                            messages = schedule.get("messages", [])
                            if any(msg.strip() in ["# ë¸”ë£¸ë²„ê·¸", "# ì˜¤ëŠ˜ì˜ ì´ìŠˆ", "# ê¸ˆìœµë‰´ìŠ¤"] for msg in messages):
                                rooms.append({
                                    "bot_name": bot_name,
                                    "channel_id": channel_id,
                                    "room_name": channel_data.get("room_name", "Unknown Room")
                                })
                                break
        except Exception as e:
            logger.error(f"Error getting rooms for Bloomberg: {e}")

        return rooms

    async def send_message_to_room(self, room_info: Dict[str, str], message: str) -> bool:
        try:
            bot_name = room_info["bot_name"]
            channel_id = room_info["channel_id"]
            room_name = room_info.get("room_name", "Unknown")

            logger.info(f"Preparing to send message to {room_name} (channel: {channel_id})")

            # context êµ¬ì„± (writerëŠ” send_message_responseì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì°¾ìŒ)
            context = {
                'bot_name': bot_name,
                'channel_id': channel_id,
                'room': room_name
            }
            from core.utils.send_message import send_message_response
            await send_message_response(context, message)
            logger.info(f"Bloomberg news sent to {bot_name}/{room_name}")
            return True
        except Exception as e:
            logger.error(f"Error sending message to {room_info.get('room_name', 'Unknown')}: {e}")
            return False

    async def broadcast_bloomberg_news(self, include_summary=True):
        """
        ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ë“±ë¡ëœ ëª¨ë“  ë°©ì— ë¶„í• í•˜ì—¬ ë°œì†¡í•˜ëŠ” ë©”ì„œë“œ
        include_summary: ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ê¸°ì‚¬ ìš”ì•½ ì¶”ê°€ ì—¬ë¶€
        """
        try:
            # ìµœì‹  ë¸”ë£¸ë²„ê·¸
            messages = await self.scraper.get_latest_issues_split(include_summary=include_summary)
            if not messages or len(messages) == 0 or messages[0].startswith("ìµœì‹  ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."):
                logger.warning("No Bloomberg issues found to send")
                return

            # ë°œì†¡í•  ë°© ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            schedule_data = await self.load_schedule_rooms()
            target_rooms = self._get_rooms_for_bloomberg(schedule_data)

            if not target_rooms:
                logger.warning("No target rooms found for Bloomberg news")
                return

            logger.info(f"Broadcasting Bloomberg news to {len(target_rooms)} rooms")

            # ê° ë°©ì— ë©”ì‹œì§€ ë°œì†¡ (ë©”ì‹œì§€ë‹¹ 1ì´ˆ ê°„ê²©)
            for room in target_rooms:
                logger.info(f"Starting to send messages to room: {room['room_name']}")
                for i, msg in enumerate(messages):
                    logger.info(
                        f"Attempting to send message {i + 1}/{len(messages)} to {room['room_name']}: {msg[:30]}...")
                    success = await self.send_message_to_room(room, msg)
                    logger.info(
                        f"Message {i + 1}/{len(messages)} to {room['room_name']}: {'Success' if success else 'Failed'}")
                    await asyncio.sleep(1)  # ë©”ì‹œì§€ ì‚¬ì´ ê°„ê²©
                logger.info(f"Completed sending messages to room: {room['room_name']}")
                await asyncio.sleep(1)  # ë°© ì‚¬ì´ ê°„ê²©

            logger.info("Bloomberg news broadcast completed")

        except Exception as e:
            logger.error(f"Error in broadcasting Bloomberg news: {e}")


# LOA.iì— ì—°ê²°í•  í•¸ë“¤ëŸ¬ í•¨ìˆ˜
async def handle_bloomberg_news(prompt=None):
    """
    '# ë¸”ë£¸ë²„ê·¸' ë˜ëŠ” '# ì˜¤ëŠ˜ì˜ ì´ìŠˆ' ëª…ë ¹ì–´ ì²˜ë¦¬ í•¨ìˆ˜
    """
    try:
        scraper = BloombergScraper()
        messages = await scraper.get_latest_issues_split(include_summary=False)

        if not messages or len(messages) == 0:
            logger.error("ìµœì‹  ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""

        # ì²« ë²ˆì§¸ ë©”ì‹œì§€ë§Œ ë°˜í™˜ (ì œëª© + ìš”ì•½ + ë§í¬)
        return messages[0] + "\n\n(ìƒì„¸ ë‚´ìš©ì€ ì±„íŒ…ë°© ìŠ¤ì¼€ì¤„ ë©”ì‹œì§€ë¡œ ë°œì†¡ë©ë‹ˆë‹¤.)"
    except Exception as e:
        logger.error(f"Error in bloomberg service: {e}")
        logger.error(f"ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return ""


async def scheduled_send_bloomberg_messages(bot_name, channel_id, room_name, messages):
    """
    ë¸”ë£¸ë²„ê·¸ ë©”ì‹œì§€ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§í•˜ì—¬ ë³´ë‚´ëŠ” í•¨ìˆ˜
    """
    try:
        # core.globalsë¥¼ ì§ì ‘ ì„í¬íŠ¸
        from core import globals as g

        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì¶”ì  ë©”ì»¤ë‹ˆì¦˜
        sent_messages = set()

        # ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸ - ìˆœí™˜ ì°¸ì¡° ë°©ì§€
        from core.utils.send_message import send_message

        # ê° ë©”ì‹œì§€ì— ëŒ€í•´ ì§€ì—°ì„ ë‘ê³  ì „ì†¡
        for i, message in enumerate(messages):
            # ê°™ì€ ë©”ì‹œì§€ëŠ” í•œ ë²ˆë§Œ ë³´ë‚´ê¸° (ì¼ë¶€ ë¬¸ìì—´ë¡œë§Œ ì²´í¬)
            message_key = f"{i}_{message[:20]}"
            if message_key in sent_messages:
                logger.info(f"Skipping duplicate message {i + 1}/{len(messages)}")
                continue

            # ê° ë©”ì‹œì§€ ë°œì†¡ ì‚¬ì´ì— ì‹œê°„ ê°„ê²© ì¶”ê°€
            # ì²« ë©”ì‹œì§€ëŠ” ë¹ ë¥´ê²Œ, ë‚˜ë¨¸ì§€ëŠ” ë” ê¸´ ê°„ê²©ìœ¼ë¡œ
            delay = 1.5 if i < 2 else 3
            await asyncio.sleep(delay)

            context = {
                'bot_name': bot_name,
                'channel_id': channel_id,
                'room': room_name
            }
            from core.utils.send_message import send_message_response
            try:
                await send_message_response(context, message)
                sent_messages.add(message_key)
                logger.info(f"Scheduled message {i + 1}/{len(messages)} sent to {room_name}")
            except Exception as e:
                logger.error(f"Failed to send message {i + 1}/{len(messages)}: {e}")
                # í•œ ë²ˆ ë” ì‹œë„
                try:
                    await asyncio.sleep(2)
                    await send_message_response(context, message)
                    sent_messages.add(message_key)
                    logger.info(f"Retry successful for message {i + 1}/{len(messages)}")
                except Exception as retry_e:
                    logger.error(f"Retry also failed for message {i + 1}/{len(messages)}: {retry_e}")

        logger.info(f"Completed sending {len(sent_messages)}/{len(messages)} bloomberg messages to {room_name}")

    except Exception as e:
        logger.error(f"Error in scheduled_send_bloomberg_messages: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")


# ìŠ¤ì¼€ì¤„ ë°œì†¡ í•¨ìˆ˜ - ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œ
async def scheduled_bloomberg_news():
    """
    ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ í†µí•´ ì •í•´ì§„ ì‹œê°„ì— ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ë°œì†¡í•˜ëŠ” í•¨ìˆ˜
    """
    service = BloombergService()
    # ê¸°ì‚¬ ìš”ì•½ í¬í•¨í•˜ì—¬ ë°œì†¡ (ê¸°ë³¸ê°’: True)
    await service.broadcast_bloomberg_news(include_summary=True)


# ì±„íŒ…ë°© ëª…ë ¹ì–´ ì²˜ë¦¬ í•¨ìˆ˜ (ë¶„í•  ë°œì†¡)
async def process_bloomberg_command(received_message):
    """
    '# ë¸”ë£¸ë²„ê·¸' ëª…ë ¹ì–´ ì²˜ë¦¬ í•¨ìˆ˜
    ì²« ë²ˆì§¸ ë©”ì‹œì§€ ì¦‰ì‹œ ì‘ë‹µ í›„ ë‚˜ë¨¸ì§€ ë¶„í•  ë°œì†¡
    """
    try:
        channel_id = received_message.get("channel_id")
        room_name = received_message.get("room")
        bot_name = received_message.get("bot_name", "")

        # ë¡œê·¸ ì¶”ê°€
        logger.info(f"Processing Bloomberg command for {bot_name}/{channel_id}/{room_name}")

        # ë¸”ë£¸ë²„ê·¸ ì´ìŠˆ ê°€ì ¸ì˜¤ê¸° - ìš”ì•½ í¬í•¨
        scraper = BloombergScraper()

        # ìµœì‹  ê¸°ì‚¬ URL ê°€ì ¸ì˜¤ê¸°
        article_url = await scraper.get_latest_issue_url()
        if not article_url:
            logger.error("ìµœì‹  ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""

        # ê¸°ì‚¬ ë‚´ìš© íŒŒì‹±
        article_data = await scraper.parse_article(article_url)
        if not article_data:
            logger.error("ì˜¤ëŠ˜ì ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""

        # ë©”ì‹œì§€ í¬ë§·íŒ…
        messages = scraper.format_for_kakao(article_data)

        # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ê¸°ì‚¬ ìš”ì•½ ì¶”ê°€
        article_summary = await scraper.create_article_summary()
        closing_message = "ë¸”ë£¸ë²„ê·¸ ì˜¤ëŠ˜ì˜ 5ê°€ì§€ ì´ìŠˆì…ë‹ˆë‹¤."
        if article_summary:
            closing_message += article_summary
        messages.append(closing_message)

        if not messages or len(messages) == 0:
            logger.error("ìµœì‹  ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""

        # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ë¦¬í„´
        return messages

    except Exception as e:
        logger.error(f"Error in process_bloomberg_command: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        logger.error("ë¸”ë£¸ë²„ê·¸ ì´ìŠˆë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return ""


async def send_remaining_messages(bot_name, channel_id, room_name, messages):
    """
    ë‚˜ë¨¸ì§€ ë©”ì‹œì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°œì†¡í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # context êµ¬ì„± (writerëŠ” send_message_responseì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ì°¾ìŒ)
        context = {
            'bot_name': bot_name,
            'channel_id': channel_id,
            'room': room_name
        }
        from core.utils.send_message import send_message_response
        for i, message in enumerate(messages):
            await send_message_response(context, message)
            logger.info(f"Sent follow-up message {i + 1}/{len(messages)} to {room_name}")
            await asyncio.sleep(1.5)  # ë©”ì‹œì§€ ê°„ ì§€ì—°ì‹œê°„
    except Exception as e:
        logger.error(f"Error sending remaining messages: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
