import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
import re
from urllib.parse import urlparse, parse_qs, unquote
from core.logger import logger
from core.utils.send_message import send_message_response
from core.utils.auth_utils import is_admin
from services.llm_fallback_service import call_llm_with_fallback
from core.globals import apply_kakao_readmore

# ì±„ë„ë³„ ë§ˆì§€ë§‰ ìš”ì•½ ì‹œê°„ ê¸°ë¡
last_summary_time_web = {}


def extract_custom_model(prompt: str) -> tuple[str, dict | None]:
    """
    í”„ë¡¬í”„íŠ¸ ë‚´ì— 'name:model' í˜•íƒœë¡œ ì§€ì •ëœ LLM ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¶„ë¦¬í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    match = re.search(r'\b([a-zA-Z0-9_-]+):([a-zA-Z0-9.\-_]+)', prompt)
    if match:
        name = match.group(1).strip()
        model = match.group(2).strip()
        cleaned_prompt = re.sub(r'\b[a-zA-Z0-9_-]+:[a-zA-Z0-9.\-_]+', '', prompt).strip()
        return cleaned_prompt, {"name": name, "model": model}
    return prompt, None


def is_valid_url(url: str) -> bool:
    return url.startswith("http://") or url.startswith("https://")


def extract_url_from_text(text: str) -> str | None:
    match = re.search(r'(https?://\S+)', text)
    return match.group(0) if match else None


async def resolve_redirect_url(url: str) -> str:
    """
    ë„¤ì´ë²„ ë‹¨ì¶• URL(naver.me, link.naver.com)ì„ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… ëª©ì ì§€ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    try:
        parsed = urlparse(url)

        # ì²« ë²ˆì§¸ ë¦¬ë””ë ‰ì…˜: naver.me â†’ link.naver.com
        if parsed.netloc == 'naver.me':
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=5) as response:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")
                    meta_refresh = soup.find("meta", attrs={"http-equiv": "refresh"})
                    if meta_refresh:
                        content = meta_refresh["content"]
                        match = re.search(r'url=(.+)', content, re.IGNORECASE)
                        if match:
                            intermediate_url = match.group(1).strip()
                            logger.info(f"[REDIRECT_NAVER_ME] ì²« ë²ˆì§¸ ë‹¨ê³„ í•´ì„ ì„±ê³µ â†’ {intermediate_url}")
                            url = intermediate_url  # URL ê°±ì‹  í›„ ë‹¤ì‹œ ì²˜ë¦¬

                            # ë‹¤ì‹œ íŒŒì‹±í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
                            parsed = urlparse(url)

        # ë‘ ë²ˆì§¸ ë¦¬ë””ë ‰ì…˜: link.naver.com â†’ ì‹¤ì œ URL
        if parsed.netloc == 'link.naver.com' and parsed.path == '/bridge':
            qs = parse_qs(parsed.query)
            if 'url' in qs:
                final_url = unquote(qs['url'][0])
                if final_url.startswith(('http://', 'https://')):
                    logger.info(f"[REDIRECT_LINK_NAVER] ë‘ ë²ˆì§¸ ë‹¨ê³„ í•´ì„ ì„±ê³µ â†’ {final_url}")
                    return final_url

        # ì¼ë°˜ì ì¸ ë¦¬ë””ë ‰íŠ¸ ì²˜ë¦¬
        async with aiohttp.ClientSession() as session:
            async with session.get(url, allow_redirects=True, timeout=5) as response:
                final_url = str(response.url)
                logger.info(f"[REDIRECT_RESOLVED] ì¼ë°˜ ë¦¬ë””ë ‰íŠ¸ í•´ì„ ì„±ê³µ â†’ {final_url}")
                return final_url

    except Exception as e:
        logger.warning(f"[REDIRECT_RESOLVE_FAIL] ë¦¬ë””ë ‰ì…˜ í•´ì„ ì‹¤íŒ¨ â†’ {e}")
        return url


async def fetch_webpage_content(url: str) -> tuple[str, str] | None:
    """
    ì§€ì •ëœ URLì˜ HTMLì„ ê°€ì ¸ì™€ ì œëª©ê³¼ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                if response.status != 200:
                    return None
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                title = soup.title.string.strip() if soup.title else "ì œëª© ì—†ìŒ"
                paragraphs = soup.find_all('p')
                content = " ".join(p.get_text(strip=True) for p in paragraphs)
                return title, content[:10000]
    except Exception as e:
        logger.error(f"[FETCH_ERROR] ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None


def is_youtube_playlist(url: str) -> bool:
    return "youtube.com/playlist" in url


async def summarize_webpage_with_llm(
        url: str,
        title: str,
        content: str,
        requested_provider: dict | None = None,
        is_admin_user: bool = False,
        context: dict | None = None
) -> str:
    """
    ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë”ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"""ë‹¤ìŒì€ ì›¹í˜ì´ì§€ ì •ë³´ì…ë‹ˆë‹¤:\n\nURL: {url}\nì œëª©: {title}\n\në‚´ìš©:\n{content}\n\nì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. """

    if is_youtube_playlist(url):
        system_prompt = (
            "ë‹¹ì‹ ì€ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
            "ì´ëª¨í‹°ì½˜ì„ ì ì ˆíˆ í™œìš©í•˜ì—¬ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."
            "**, ## ë§ˆí¬ë‹¤ìš´ ë¬¸ìëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
            ""
            "ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•´ ì£¼ì„¸ìš”."
            ""
            "[{í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ì œëª©}]"
            ""
            "1. {ì²«ë²ˆì¨° ìŒì•…}"
            "2. {ë‘ë²ˆì¨° ìŒì•…}"
            "3. {ì„¸ë²ˆì¨° ìŒì•…}"
        )
    else:
        system_prompt = (
            "ë‹¹ì‹ ì€ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ í˜ì´ì§€ë¥¼ ì§ì ‘ ì—´ì§€ ì•Šì•„ë„ í•µì‹¬ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½í•´ì£¼ì„¸ìš”. "
            "ê¸°ì‚¬ ì¶œì²˜ì— ëŒ€í•´ì„œëŠ” ìì„¸íˆ ì„¤ëª…í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."
            "ì´ëª¨í‹°ì½˜ ë“±ì„ í™œìš©í•´ì„œ ê°€ë…ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”."
            "**, ## ë§ˆí¬ë‹¤ìš´ ë¬¸ìëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
            "ì œëª©ì€ ì´ë¯¸ ê°€ì¥ ìƒë‹¨ì— í‘œì‹œë˜ë¯€ë¡œ ë”°ë¡œ ì–¸ê¸‰í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."
            "ì•ˆë…•í•˜ì„¸ìš” ê°™ì€ ì¸ì‚¬ë§ì´ë‚˜ ìš”ì•½í•´ ë“œë¦´ê»˜ìš”! ë€ ë§ì€ í•  í•„ìš” ì—†ì´ ê·¸ëƒ¥ ìš”ì•½ì„ í•´ì£¼ì„¸ìš”."
            "ìš”ì•½ ë‚´ìš©ì„ ì—¬ëŸ¬ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤ë©´ ê° ì„¹ì…˜ì„ ëŒ€í‘œí•˜ëŠ” ì´ëª¨í‹°ì½˜ìœ¼ë¡œ êµ¬ë¶„í•´ ì£¼ì„¸ìš”."
            "ê° ì„¹ì…˜ì˜ íƒ€ì´í‹€ë§Œ ë´ë„ ëŒ€ëµì ì¸ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ì„¸ë¶„í™” í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤."
            "ê° ì„¹ì…˜ì˜ ë‚´ìš©ì€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ê°„ëµí•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
            "ê°€ì¥ ë§ˆì§€ë§‰ ì„¹ì…˜ì€ 'ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸'ë¡œ ë§ˆë¬´ë¦¬ í•´ì£¼ì„¸ìš”."
        )

    received_message = {
        "text": prompt,
        "bot_name": context.get("bot_name", ""),
        "video_summary": False,
        "source": url
    }
    if requested_provider and is_admin_user:
        providers = [{
            "name": requested_provider["name"],
            "model": requested_provider["model"],
            "timeout": 30,
            "retry": 0,
            "system_prompt": system_prompt
        }]
    else:
        providers = [
            {"name": "grok", "model": "grok-3-latest", "timeout": 30, "retry": 0, "system_prompt": system_prompt},
            {"name": "gemini", "model": "gemini-1.5-pro", "timeout": 30, "retry": 0, "system_prompt": system_prompt},
            {"name": "openai", "model": "gpt-4o", "timeout": 30, "retry": 0, "system_prompt": system_prompt},
            {"name": "gemini-flash", "model": "gemini-1.5-flash", "timeout": 30, "retry": 0,
             "system_prompt": system_prompt},
            {"name": "deepseek", "model": "deepseek-chat", "timeout": 30, "retry": 0, "system_prompt": system_prompt}
        ]
    return await call_llm_with_fallback(received_message, prompt, providers)


async def handle_webpage_summary(prompt: str, context: dict | None = None) -> str:
    """
    ìˆ˜ë™ìœ¼ë¡œ ìš”ì²­ëœ ì›¹í˜ì´ì§€ ìš”ì•½ ëª…ë ¹ì–´ ì²˜ë¦¬.
    """
    # ì»¤ìŠ¤í…€ ëª¨ë¸ì´ í¬í•¨ëœ ê²½ìš° ë¶„ë¦¬
    url, provider = extract_custom_model(prompt)
    if not is_valid_url(url):
        return "âš ï¸ ìœ íš¨í•œ ì›¹í˜ì´ì§€ URLì´ ì•„ë‹™ë‹ˆë‹¤."

    logger.info(f"[HANDLE_WEBPAGE] ìš”ì•½ ì‹œì‘ â†’ {url}")
    # URL ë‹¨ì¶• ì„œë¹„ìŠ¤ì¸ ê²½ìš° ìµœì¢… ë¦¬ë””ë ‰ì…˜ URLë¡œ ë³€í™˜
    url = await resolve_redirect_url(url)
    # ë³¸ë¬¸ í¬ë¡¤ë§
    result = await fetch_webpage_content(url)
    if not result:
        return "âš ï¸ ì›¹í˜ì´ì§€ ìš”ì•½ì„ ìœ„í•œ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    title, content = result
    is_admin_user = is_admin(context.get("channel_id"), context.get("user_hash")) if context else False
    if provider and not is_admin_user:
        provider = None

    summary = await summarize_webpage_with_llm(url, title, content, provider, is_admin_user, context)
    # kakao_readmore ì ìš©ì„ ìœ„í•œ config ì •ì˜
    config = {}
    if context:
        import core.globals as g
        bot_name = context.get("bot_name", "")
        channel_id = str(context.get("channel_id"))
        room_config = g.schedule_rooms.get(bot_name, {}).get(channel_id, {})
        config = room_config.get("webpage_summary", {})
    # kakao_readmore ì ìš©
    kakao_readmore = config.get("kakao_readmore", {})
    kakao_type = kakao_readmore.get("type", "lines")
    kakao_value = int(kakao_readmore.get("value", 1))
    full_message = f"ğŸ“ ì›¹í˜ì´ì§€ ìš”ì•½ ({title})\n\n{summary}"    
    full_message = apply_kakao_readmore(full_message, kakao_type, kakao_value)

    # ìˆ˜ë™ ê¸°ë¡ ë¡œê¹…
    if context:
        try:
            import core.globals as g
            async with g.db_pool.acquire() as conn:
                async with conn.cursor() as cur:
                    await cur.execute(
                        """
                        INSERT INTO kb_webpage_summary_logs 
                        (channel_id, user_hash, type, url, created_at)
                        VALUES (%s, %s, %s, %s, NOW())
                        """,
                        (
                            context.get("channel_id"),
                            context.get("user_hash"),
                            "command",
                            url
                        )
                    )
        except Exception as e: 
            logger.error(f"[DB_LOG_FAIL] ì›¹í˜ì´ì§€ ìš”ì•½ ê¸°ë¡ ì‹¤íŒ¨: {e}")
            
    return full_message


async def process_auto_webpage_summary(message: dict) -> str | None:
    """
    ìë™ ê²€ì¶œëœ ì›¹í˜ì´ì§€ ë§í¬ì— ëŒ€í•´ ì¼ì • ì œí•œê³¼ ì¿¨ë‹¤ìš´ì„ ì ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìš”ì•½ì„ ì‹¤í–‰.
    """
    try:
        import core.globals as g
        channel_id = message.get("channel_id")
        user_hash = message.get("user_hash")
        text = message.get("text")
        # í…ìŠ¤íŠ¸ì—ì„œ URL ì¶”ì¶œ
        url = extract_url_from_text(text)
        if not url or not is_valid_url(url):
            return None
        # URL ë‹¨ì¶• ì„œë¹„ìŠ¤ì¸ ê²½ìš° ìµœì¢… ë¦¬ë””ë ‰ì…˜ URLë¡œ ë³€í™˜
        url = await resolve_redirect_url(url)

        # ë°© ì„¤ì • í™•ì¸
        room_config = g.schedule_rooms.get(message.get("bot_name", ""), {}).get(str(channel_id), {})
        config = room_config.get("webpage_summary", {})
        if not config.get("enabled", False):
            return None
        detection = config.get("auto_detection", {})
        if not detection.get("enabled", True):
            return None

        # ì¼ì¼ ì œí•œ ì²´í¬
        limit = detection.get("daily_limit", 0)
        if limit > 0:
            async with g.db_pool.acquire() as conn:
                async with conn.cursor() as cur:
                    await cur.execute(
                        """
                        SELECT COUNT(*) FROM kb_webpage_summary_logs
                        WHERE channel_id = %s AND DATE(created_at) = %s AND type = 'auto'
                        """,
                        (channel_id, datetime.now().strftime("%Y-%m-%d"))
                    )
                    count = (await cur.fetchone())[0]
                    if count >= limit:
                        return None

        # ì¿¨ë‹¤ìš´ ì ìš©
        cooldown = detection.get("cooldown_seconds", 60)
        now = datetime.now()
        if channel_id in last_summary_time_web:
            diff = (now - last_summary_time_web[channel_id]).total_seconds()
            if diff < cooldown:
                return None
        last_summary_time_web[channel_id] = now

        # ëŒ€ê¸° ë©”ì‹œì§€ ì „ì†¡
        if detection.get("show_waiting_message", True):
            await send_message_response(context=message, message="â³ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ìˆì–´ìš”...")

        # ìš”ì•½ ì‹¤í–‰
        summary = await handle_webpage_summary(url, message)
        # ìë™ ë¡œê·¸ ê¸°ë¡
        try:
            async with g.db_pool.acquire() as conn:
                async with conn.cursor() as cur:
                    await cur.execute(
                        """
                        INSERT INTO kb_webpage_summary_logs 
                        (channel_id, user_hash, type, url, created_at)
                        VALUES (%s, %s, %s, %s, NOW())
                        """,
                        (channel_id, user_hash, "auto", url)
                    )
        except Exception as e:
            logger.error(f"[AUTO_LOG_FAIL] ì›¹í˜ì´ì§€ ìë™ ìš”ì•½ ê¸°ë¡ ì‹¤íŒ¨: {e}")

        return summary
    except Exception as e:
        logger.error(f"[AUTO_SUMMARY_FAIL] ì˜ˆì™¸ ë°œìƒ: {e}")
        return None
